{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for a painless q-learning tutorial\n",
    "\n",
    "- refer to \n",
    "\n",
    "        http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    "\n",
    "        https://ai.intel.com/demystifying-deep-reinforcement-learning\n",
    "        \n",
    "- build a simple AI based on `numpy`.\n",
    "- write this just for better understand qlearning.\n",
    "- Author: [bailiqun](https://github.com/qlearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    本教程通过一个直观简单的例子介绍qlearning，本例子中描述的是一个机器人是如何走出五个房间。\n",
    "可以看到***Agent***（智能体，也就是执行操作的机器人）不经过人为干预自行找到出口。\n",
    "\n",
    "<img src=\"./img/modeling_environment_clip_image002a.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     假设我们有5个房间，各个房间相互连接如上图所示，我们的房间号依次为0至4，房间外我们定为数字5。\n",
    "我们可以将房间表示为图，每个房间是一个节点，节点间的连接表示门。\n",
    "\n",
    "     在这个例子中，我们把Agent放到随机的房间，并走出房间。换句话说，目标是走到数字5.为了让机器\n",
    "     \n",
    "人可以走到终点，我们要设置一些***reward***.直接通向5的数字节点添加奖励为100，不直接连接数字5的门添加奖\n",
    "\n",
    "励0,不相连的门添加代价-1.\n",
    "\n",
    "<img src=\"./img/map1a.gif\" />\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Markov Decision Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   如何正式的描述强化学习问题，最为常见的方法是把他表示为马尔科夫决策链。假设你是个机器人(Agent)，\n",
    "   被放置到`环境`(enviroment)中(放置到房间中),Agent会在环境执行`动作`(actions),这些动作有时会\n",
    "   获取一些`奖励`(reward).动作作用到环境中，会产生新的状态，从而一直循环下去。如何选择机器人的动作\n",
    "   我们称之为`策略`(policy).\n",
    "   \n",
    "   <img src = \"./img/reinforcement-learning-an-introduction.png\" />\n",
    "\n",
    "                    左：强化学习问题                         右：马尔科夫决策过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./img/Q-learning-algorithm-example.png\" />\n",
    "\n",
    "- Qlearning可以简单的归纳为下面公式\n",
    "\n",
    "    **Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]***\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_q (q):\n",
    "    '''\n",
    "    更好的格式化打印矩阵\n",
    "    '''\n",
    "    for row in range(q.shape[0]):\n",
    "        print('state %d |'% row,end = '')\n",
    "        for col in range(q.shape[1]):\n",
    "            print(\"%6d \" % int(q[row,col]),end='')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lr = 0.5\n",
    "gamma = 0.8\n",
    "\n",
    "q = np.zeros([6, 6])\n",
    "r = np.array([[-1, -1, -1, -1,  0,  -1], \n",
    "              [-1, -1, -1,  0, -1, 100], \n",
    "              [-1, -1, -1,  0, -1, -1], \n",
    "              [-1,  0,  0, -1,  0, -1], \n",
    "              [0,  -1, -1,  0, -1, 100], \n",
    "              [-1,  0, -1, -1,  0, 100]])\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    # one episode\n",
    "    state = random.randint(0, 5)\n",
    "    positive_action = np.where(r[state]>=0)\n",
    "    \n",
    "    action = positive_action[random.randint(0, len(positive_action) - 1)]\n",
    "    reward = r[state,action]\n",
    "    new_state = action\n",
    "    \n",
    "    q[state, action] = q[state, action] + lr * (reward + gamma * q[next_state].max() - q[state, action]) \n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Q---------------------------\n",
      "\n",
      "state 0 |     0      0      0      0    399      0 \n",
      "\n",
      "state 1 |     0      0      0    399      0    499 \n",
      "\n",
      "state 2 |     0      0      0    399      0      0 \n",
      "\n",
      "state 3 |     0    399    399      0    399      0 \n",
      "\n",
      "state 4 |   399      0      0    399      0    499 \n",
      "\n",
      "state 5 |     0    399      0      0    399    499 \n",
      "\n",
      "\n",
      "----------------------Vertify-----------------------\n",
      "\n",
      "0 -> 4 -> 5 (end)\r\n",
      "\n",
      "1 -> 5 (end)\r\n",
      "\n",
      "2 -> 3 -> 1 -> 5 (end)\r\n",
      "\n",
      "3 -> 1 -> 5 (end)\r\n",
      "\n",
      "4 -> 5 (end)\r\n",
      "\n",
      "5 (end)\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------Q---------------------------\\n')\n",
    "pretty_q(q)\n",
    "\n",
    "\n",
    "print('\\n----------------------Vertify-----------------------\\n')\n",
    "for i in range(6):\n",
    "    state = i\n",
    "    count = 0\n",
    "    while (state != 5):\n",
    "        # prevent endless loop\n",
    "        if count > 20:\n",
    "            print('失败，无法走到出口')\n",
    "            break\n",
    "        \n",
    "        q_max_action = np.argmax(q[state])\n",
    "        next_state = q_max_action\n",
    "        \n",
    "        print(\"%d -> \" % (state),end='')\n",
    "        state = next_state\n",
    "        count = count + 1\n",
    "    print(\"5 (end)\\r\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/map1a.gif\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
